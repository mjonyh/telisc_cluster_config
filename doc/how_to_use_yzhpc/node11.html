<!DOCTYPE HTML>

<!--Converted with LaTeX2HTML 2019.2 (Released June 5, 2019) -->
<HTML lang="EN">
<HEAD>
<TITLE>Using Simple MPI</TITLE>
<META NAME="description" CONTENT="Using Simple MPI">
<META NAME="keywords" CONTENT="how_to_use_yzhpc">
<META NAME="resource-type" CONTENT="document">
<META NAME="distribution" CONTENT="global">

<META HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=utf-8">
<META NAME="viewport" CONTENT="width=device-width, initial-scale=1.0">
<META NAME="Generator" CONTENT="LaTeX2HTML v2019.2">

<LINK REL="STYLESHEET" HREF="how_to_use_yzhpc.css">

<LINK REL="next" HREF="node12.html">
<LINK REL="previous" HREF="node2.html">
<LINK REL="next" HREF="node12.html">
</HEAD>

<BODY >

<DIV CLASS="navigation"><!--Navigation Panel-->
<A
 HREF="node12.html">
<IMG WIDTH="37" HEIGHT="24" ALT="next" SRC="next.png"></A> 
<A
 HREF="how_to_use_yzhpc.html">
<IMG WIDTH="26" HEIGHT="24" ALT="up" SRC="up.png"></A> 
<A
 HREF="node10.html">
<IMG WIDTH="63" HEIGHT="24" ALT="previous" SRC="prev.png"></A>   
<BR>
<B> Next:</B> <A
 HREF="node12.html">Using LAMMPS</A>
<B> Up:</B> <A
 HREF="how_to_use_yzhpc.html">Welcome to YZHPC High</A>
<B> Previous:</B> <A
 HREF="node10.html">Using scp</A>
<BR>
<BR></DIV>
<!--End of Navigation Panel-->

<H1><A ID="SECTION00300000000000000000"></A><A ID="cha:using_simple_mpi"></A>
<BR>
Using Simple MPI
</H1>

<P>
This is an example of a simple MPI program that runs on multiple processors. It demonstrates the use of Slurm’s interactive mode and HPC’s openmpi setup.

<P>
A simple MPI script (test.c) is provided below for the demonstration purpose.

<P>
<PRE>
	#include "mpi.h"
	#include &lt;stdio.h&gt;

	int main (int argc, char *argv[])
	{
	    int i, rank, size, namelen;
	    char name [MPI_MAX_PROCESSOR_NAME];

	    MPI_Init (&amp;argc, &amp;argv);

	    MPI_Comm_size (MPI_COMM_WORLD, &amp;size);
	    MPI_Comm_rank (MPI_COMM_WORLD, &amp;rank);
	    MPI_Get_processor_name (name, &amp;namelen);

	    printf ("Hello World from rank %d running on %s!\n", rank, name);

	    if (rank == 0 )
	       printf ("MPI World size = %d processes\n", size);

	    MPI_Finalize ();

	}
</PRE>

<P>
Procedure to execute this script described below:
<DL>
<DT><STRONG>Load MPI</STRONG></DT>
<DD>$ module load openMPI
	
</DD>
<DT><STRONG>Compile Code</STRONG></DT>
<DD>$ mpicc -o test test.c
	
</DD>
<DT><STRONG>Make Slurm Script</STRONG></DT>
<DD>Create a script (test.sh) for submit the job as:
		<PRE>
	#!/bin/bash
	#SBATCH --job-name=mpi-job 	### jobname
	#SBATCH --partition=xeon	### partition
	#SBATCH --nodes=2 		### Number of nodes to use
	#SBATCH --ntasks-per-node=1 	### Number of tasks per node
	#SBATCH --cpus-per-task=2 	### Number of cpus per task
	#SBATCH --output=OUTPUT%J.out 	### slurm output file

	# load openMPI
	module load openMPI

	# run job
	mpirun ./test
</PRE>
	
</DD>
<DT><STRONG>Submit Job</STRONG></DT>
<DD>$ sbatch test.sh
</DD>
</DL>

<P>
<BR><HR>

</BODY>
</HTML>
