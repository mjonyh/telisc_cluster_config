\documentclass{book}
\usepackage[dvipsnames]{xcolor}
\usepackage{html,makeidx}
\usepackage{hyperref}

\usepackage{listings}
% \usepackage{amsmath}
%begin{latexonly}
\lstnewenvironment{myverbatim}{}{}
%end{latexonly}

\title{Welcome to YZHPC \\ 
{\small High Performance Computing Laboratory \\
Department of Physics \\
Shahjalal University of Science and Technology \\
Sylhet - 3114, Bangladesh}
}

%\author{Dr. Md. Enamul Hoque}
\date{Updated on \today}

\begin{document}
\maketitle
\newpage


%% Introduction chapter
\chapter{Introduction}%
\label{cha:introduction}

Configuration of the HPC:
\begin{description}
	\item[Hardware] Available hardware
		\begin{description}
			\item[Compute Node: ] 05
				\begin{itemize}
					\item CPU: Intel Xeon 4-core 2.1 GHz
					\item RAM: 8 GB 800 MHz
				\end{itemize}
			\item[Compute Node: ] 16
				\begin{itemize}
					\item CPU: Intel Core 2 Duo 2.6 GHz
					\item RAM: 4 GB 800 MHz
				\end{itemize}
			\item[Total Core: ] 52 Compute Core
			\item[Total RAM] 104 GB
		\end{description}
	\item[Software] Available Software.
		\begin{itemize}
			\item OS: Telisc OS
			\item Module Environment: lmod
			\item Task/Job Management and Schedular: SLURM
		\end{itemize}
	\item[Available Computing Packages] OpenMPI, LAMMPS
	\item[Funded by] Castle.com (Sylhet, BD), Prof. Md. Zafar Iqbal, HEQEP (CP - 4044), SUST Research Center
\end{description}

%% End of Introduction chapter

\chapter{Using shell in YZHPC}%
\label{cha:using_shell_in_yzhpc}

\section{Login}%
\label{sec:login}

Only SSH access is available to login in the system. Any SSH client from various Operating System can be used. \textcolor{red}{Additionally a web browser can be used to get login (firefox, google-chrome, Internet Explorer and Microsft edges were tested)}.

\subsection{From Linux or Mac Computer}%
\label{sub:from_linux_or_mac}

\begin{description}
	\item[Login] Open a terminal that available in your system. After that use \texttt{\$ ssh username@yzhpc.sust.edu} to login in the HPC head node.
	\item[Logout] Execute the \texttt{\$ exit} command.
\end{description}

\subsection{From Windows Computer}%
\label{sub:from_windows_computer}

A number of ssh client is available for windows machine. PuTTY is one of the best ssh client for windows machine.

\begin{enumerate}
	\item Open PuTTY from start menu.
	\item Provide username and address (i.e.\ yzhpc.sust.edu).
	\item Login to the system.
	\item Execute exit command to logout.
\end{enumerate}

\section{Useful Linux Command}%
\label{sec:useful_linux_command}

Use man COMMAND to do more with the following commands:

\begin{description}
	\item[ls] Very useful commands for documents listing in the current directory.
	\item[cd] To jump into another directory.
	\item[mkdir] Use it for creating new directory.
	\item[rm] To delete document it is used.
	\item[mv] To rename document, it is used.
	\item[cp] To copy document, it is used.
	\item[man] To view the manual of a package, it is used.
	\item[tail] Very useful command for viewing live output.
	\item[head] It shows the top few lines of a document.
	\item[less] It shows the document in visual mode.
	\item[cat] Shows all the content of a document.

\end{description} 

\section{Useful HPC command}% 
\label{sec:useful_hpc_command} 
\begin{description} 
	\item[sinfo] To view the status of available nodes.
	\item[squeue] To view the status of the jobs.
	\item[sacct] To view the details of the jobs.
	\item[module] To find the available software installed in the system.
	\item[sbatch] Submits job scripts into system for execution (queued)
	\item[scancel]Cancels a job
	\item[scontrol] Used to display Slurm state, several options only available to root
\end{description}

\section{File transfer}%
\label{sec:file_transfer}

\subsection{Using rsync}%
\label{sub:using_rsync}

\subsection{Using scp}%
\label{sub:using_scp}



%% End of Using shell in YZHPC chapter


\chapter{Using Simple MPI}%
\label{cha:using_simple_mpi}

This is an example of a simple MPI program that runs on multiple processors. It demonstrates the use of Slurm’s interactive mode and HPC’s openmpi setup.

A simple MPI script (test.c) is provided below for the demonstration purpose.

\begin{myverbatim}
	#include "mpi.h"
	#include <stdio.h>

	int main (int argc, char *argv[])
	{
	    int i, rank, size, namelen;
	    char name [MPI_MAX_PROCESSOR_NAME];

	    MPI_Init (&argc, &argv);

	    MPI_Comm_size (MPI_COMM_WORLD, &size);
	    MPI_Comm_rank (MPI_COMM_WORLD, &rank);
	    MPI_Get_processor_name (name, &namelen);

	    printf ("Hello World from rank %d running on %s!\n", rank, name);

	    if (rank == 0 )
	       printf ("MPI World size = %d processes\n", size);

	    MPI_Finalize ();

	}
\end{myverbatim}

Procedure to execute this script described below:
\begin{description}
	\item[Load MPI] \$ module load openMPI
	\item[Compile Code] \$ mpicc -o test test.c
	\item[Make Slurm Script] Create a script (test.sh) for submit the job as:
		\begin{myverbatim}
	#!/bin/bash
	#SBATCH --job-name=mpi-job 	### jobname
	#SBATCH --partition=xeon	### partition
	#SBATCH --nodes=2 		### Number of nodes to use
	#SBATCH --ntasks-per-node=1 	### Number of tasks per node
	#SBATCH --cpus-per-task=2 	### Number of cpus per task
	#SBATCH --output=OUTPUT%J.out 	### slurm output file

	# load openMPI
	module load openMPI

	# run job
	mpirun ./test

	\end{myverbatim}
	\item[Submit Job] \$ sbatch test.sh
\end{description}

\chapter{Using LAMMPS}%
\label{cha:using_lammps}

To run Molecular Dynamics (MD) simulation in the YZ facility you have to write an input script (Ex. in.filename) for LAMMPS and a batch script(Ex. job.sh).

The input script contains all the information about the simulation and the batch file contains all the information about how many nodes and core you are gonna use.

\section{LAMMPS Input File}%
\label{sec:lammps_input_file}

Under development

\section{Running LAMMPS}%
\label{sec:running_lammps}

For an input file (consider in.filename), you need to make a job submission file (say job.sh) as follows:

\begin{myverbatim}
	#!/bin/bash
	#SBATCH --job-name=lammps-job 	### jobname
	#SBATCH --partition=xeon	### partition
	#SBATCH --nodes=4 		### Number of nodes to use
	#SBATCH --ntasks-per-node=1 	### Number of tasks per node
	#SBATCH --cpus-per-task=4 	### Number of cpus per task
	#SBATCH --output=OUTPUT%J.out 	### slurm output file

	# load lammps
	module load lammps

	# input and output file name
	INPUT=in.filename
	OUTPUT=filename.out

	# run job
	mpirun lmp -in ${INPUT} -log ${OUTPUT}

\end{myverbatim}

Submit the job with slurm as in terminal:

\begin{myverbatim}
	$ sbatch job.sh
\end{myverbatim}


\chapter{Using Gaussian(g09)}%
\label{cha:using_gaussian_g09_}

Gaussian is a software application used for electronic structure modeling. In order to run Gaussian09 on YZHPC, you must be a part of the ‘g09’ YZHPC group.  Request to be added to this group by sending an email to yzhpc@sust.edu.  

\section{Setting up Gaussian09}%
\label{sec:setting_up_gaussian09}

Before running Gaussian09, you should set up your environment with the following command.

If you are using the bash shell, enter:

\begin{myverbatim}
	source /etc/g09/g09setup
\end{myverbatim}

\section{Running Gaussian(g09)}%
\label{sec:running_gaussian_g09_}

To run the scalar version of Gaussian09, use the command g09. To run the parallel, also known as Linda, version of Gaussian09.

While using the parallel version of Gaussian09, you will need to specify the number of processors to be used in the ‘\%nprocshared’ as a link 0 command in your Gaussian program. The following should be the first few lines of your Gaussian input file:

\begin{myverbatim}
	%NProcShared=4
	%mem=120MW
	%chk=TYRO_PUB_CHEM_ZITT_enrg_TD_dft.chk
	# td=(nstates=10) ..
\end{myverbatim}

\textbf{Note:} It indicates the compute nodes will have 4 core each. The path of the chk is recommended to be some where in your home directory.

\begin{myverbatim}
	#!/bin/bash
	#SBATCH --job-name=g09-bench 	### jobname
	#SBATCH --partition=xeon	### partition
	#SBATCH --nodes=4 		### Number of nodes to use
	#SBATCH --ntasks-per-node=1 	### Number of tasks per node
	#SBATCH --cpus-per-task=4 	### Number of cpus per task
	#SBATCH --output=OUTPUT%J.out 	### slurm output file

	# Name of your input file ie INPUT.com
	JobFile=INPUT

	# This creates a list of nodes that you job received to run on
	LindaList=./nodes_linda.$SLURM_JOBID
	touch $LindaList

	# This creates a jobfile
	JobName=./${JobFile}${SLURM_JOBID}.com
	touch $JobName

	# Create a list of hostnames and save it
	srun hostname -s | sort -u > $LindaList

	# Tell linda to use ssh
	export GAUSS_LFLAGS=' -opt "Tsnet.Node.lindarsharg: ssh"'

	# Read the contents of the machine file and put it in the job file
	workers="%LindaWorkers="$(cat $LindaList | tr "\n" "," | sed "s/,$//")

	# Write that out to the job file
	cat <(echo "${workers}") ./$JobFile.com > $JobName

	source /etc/g09/g09setup.sh

	g09  ${JobFile}${SLURM_JOBID}.com   ${JobFile}${SLURM_JOBID}.out

	echo "Job Complete /$"
\end{myverbatim}

\textbf{Note:} It is recommended to use #SBATCH --nodes=4. One shall choice a output file name that corresponds to the input file. Suppose, you have a input file called INPUT.com. You only need to replace JobFile=INPUT (without extension). Leave the other part of the script as it is.

\chapter{Conclusion}%
\label{cha:conclusion}

This is the Conclusion.

\end{document}
