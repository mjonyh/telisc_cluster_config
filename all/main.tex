\title{How to use YZ-HPC}
\author{
        %\large
        %\textsc{Vitaly Surazhsky}
        %    \qquad
        %\textsc{Joseph (Yossi) Gil}\thanks{Contact author}
        \mbox{}\\ %
        Department of Physics\\
        Shahjalal University of Science and Technology\\
        Sylhet - 3114, Bangladesh\\
        \mbox{}\\ %
        \normalsize
            \texttt{Support:}
        \textbar{}
            \texttt{yz-hpc}
        \normalsize
            \texttt{@sust.edu}
}
\date{}
\documentclass[11pt]{article}
%\documentclass{acmconf}

\usepackage[paper=a4paper,dvips,top=1.5cm,left=1.5cm,right=1.5cm,
    foot=1cm,bottom=1.5cm]{geometry}

\usepackage{times}
%\usepackage{graphicx}
\usepackage[fleqn]{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsopn}
\usepackage{xspace}
\usepackage{array}
\usepackage{epsfig}
\usepackage{listings}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=red,
}


\numberwithin{figure}{section}

\newcommand\CC{\Lang{\mbox{C++}}\xspace}
\newcommand\Lang[1]{\textsc{#1}}
\newcommand{\kw}[1]{\texttt{\textbf{#1}}}
\newcommand{\cd}[1]{\texttt{#1}}

\newcommand\Naturals{\ensuremath{\mathbb{N}}\xspace}
\newcommand\Integers{\ensuremath{\mathbb{Z}}\xspace}
\newcommand\Rationals{\ensuremath{\mathbb{Q}}\xspace}
\newcommand\Reals{\ensuremath{\mathbb{R}}\xspace}
\newcommand\Complex{\ensuremath{\mathbb{C}}\xspace}

\newcommand\norm[1]{\ensuremath{\lVert#1\rVert}}
\newcommand\abs[1]{\ensuremath{\lvert#1\rvert}}
\newcommand\ceil[1]{\ensuremath{\lceil#1\rceil}}
\newcommand\floor[1]{\ensuremath{\lfloor#1\rfloor}}
\newcommand\set[1]{\ensuremath{\{#1\}}}
\newcommand\angular[1]{\ensuremath{\langle#1\rangle}}

\newcommand\Norm[1]{\ensuremath{\left\lVert#1\right\rVert}}
\newcommand\Abs[1]{\ensuremath{\left\lvert#1\right\rvert}}
\newcommand\Ceil[1]{\ensuremath{\left\lceil#1\right\rceil}}
\newcommand\Floor[1]{\ensuremath{\left\lfloor#1\right\rfloor}}
\newcommand\Set[1]{\ensuremath{\left\{#1\right\}}}
\newcommand\Angular[1]{\ensuremath{\left\langle#1\right\rangle}}

\newcommand{\LOOM}{\ensuremath{\cal{LOOM}}\xspace}
\newcommand{\PolyTOIL}{\textbf{PolyTOIL}\xspace}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}[theorem]{Definition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{example}[theorem]{Example}

\newcommand\Cls[1]{\textsf{#1}}
\newcommand\Fig[1]{Figure~\ref{Figure:#1}}

\usepackage{labels} %
\usepackage{equation}
\usepackage{prog2tex}

\newenvironment{excerpt}{\begin{quote}\begin{minipage}\textwidth}{\end{minipage}\end{quote}}

\setcounter{topnumber}{0}
\setcounter{bottomnumber}{0}
\setcounter{totalnumber}{20}
\renewcommand{\textfraction}{0.01}

\begin{document}

\maketitle
 
\tableofcontents
% \begin{abstract}
% We present a programming technique for implementing
%     type safe covariance in \CC{}.
% In a sense, we implement most of Bruce's \emph{matching}
%     approach to the covariance dilemma in \CC.
% The appeal in our approach is that it relies on existing mechanisms,
%     specifically templates, and does not require any
%     modification to the existing language.
% The practical value of the technique was demonstrated
%     in its successful incorporation in a large software body.
% We identify the ingredients of a programming language
%     required for applying the technique, and discuss
%     extensions to other languages.
% \end{abstract}

\section{Getting started with the HPC}

The configuration of YZ-HPC is described below:

\begin{itemize}
    \item Hardware
        \begin{itemize}
            \item Compute Node: 05
                \begin{itemize}
                    \item CPU: Intel Xeon 4-core 2.1 GHz
                    \item RAM: 8 GB 800 MHz
                \end{itemize}
            \item Compute Node: 16
                \begin{itemize}
                    \item CPU: Intel Core 2 Duo 2.6 GHz
                    \item RAM: 4 GB 800 MHz
                \end{itemize}
            \item Total Core: 52 Compute Core
            \item Total RAM: 104 GB
        \end{itemize}
    \item Software
        \begin{itemize}
            \item OS: \href{https://www.teliscos.org}{Telisc OS}
            \item Module Environment: \href{https://lmod.readthedocs.io/en/latest/} {lmod}
            \item Task/Job Management and Schedular: \href{https://slurm.schedmd.com} {SLURM}
        \end{itemize}
    \item Computation Software: OpenMPI (Version 2.1.3 and 3.0.0)
    \item Molecuar Dynamics: LAMMPS
    \item Computational Chemistry: Gaussian (g09)
\end{itemize}

\subsection{How do I get access to the HPC}
Fill out the appropriate tab of the HPC Access Request Form. Access is
typically granted within few business days. Before requesting access, a minimum experience with the followings are expected:

\begin{itemize}
    \item How to work on Linux Terminal
    \item How to write within Linux Terminal (with vim/nano)
    \item Basics of OpenMPI
    \item File transfer tools (rsync, FileZilla, WinSCP etc.)
\end{itemize}

\subsection{YZ-HPC Documentation}
All the documentation are described brifely at \href{http://103.84.157.101} {http://yzhpc.phy.edu}. An HPC Access Request Form or Registration form is available for registering into YZ-HPC. A username with password will be sent to the user after the registration completion. Those username and password is very important for login.

NOTE: DO NOT CHANGE THE PASSWORD.

\subsection{How do I login in the system}
Only SSH access is available to login in the system. Any SSH client from
various Operating System can be used. Additionally a web browser can be used to
get login (firefox, google-chrome, Internet Explorer and Microsft edges were
tested).

\subsubsection{From web browser}

\begin{description}
    \item[url] \href{http://103.84.157.101}{http://yzhpc.phy.edu}
    \item[CLI] Click \textbf{Go to Command Line Interface}
    \item[Permission] Accept the secure access
    \item[localhost] 103.84.157.101
    \item[Port] 22
    \item[username] USERNAME
    \item[password] PASSWORD
\end{description}

A login shell will be available if everything goes fine.

\subsection{How do I run my jobs on the HPC}
See the documents below sections for basic examples of several types of jobs on
the HPC system.

\begin{itemize}
    \item HPC Sample Job: OpenMPI
    \item HPC Sample Job: LAMMPS
    \item HPC Sample Job: Gaussian
\end{itemize}

\subsection{How many jobs can I run?}
\subsection{Why are some of my jobs stuck in the queue?}

\section{How do I use TextEditor}
\label{section_editor}
By default vim and nano text editor is provided in the YZ-HPC because of their
simplicity.

\subsection{Documentation on TextEditor}
\begin{itemize}
    \item Vim (An online tutorial is available at \href{http://www.openvim.com}{here})
    \item Nano (A simple tutorial is available at \href{https://staffwww.fullcoll.edu/sedwards/Nano/IntroToNano.html}{here}) 
\end{itemize}

\section{How do I transfer file into/from YZ-HPC}
\label{section_transfer}
Any standard SSH tool can be used to transfer files between HPC and client
computer. The rsync, WinSCP are Filezilla very useful tools.

\subsection{Documentation on File Transfer}

\section{HPC Sample Job: OpenMPI}
\subsection{Overview}
This document shows a very simple "Hello, World!" type program using OpenMPI
libraries, adapted from MPI Tutorial: MPI Hello World.

mpi\_hw.c

\begin{lstlisting}[frame=single]
#include <mpi.h>
#include <stdio.h>
 
int main(int argc, char** argv) {
  MPI_Init(NULL, NULL);
  int world_size;
  MPI_Comm_size(MPI_COMM_WORLD, &world_size);
  int world_rank;
  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);
  char processor_name[MPI_MAX_PROCESSOR_NAME];
  int name_len;
  MPI_Get_processor_name(processor_name, &name_len);
  printf("Hello world from processor %s, rank %d"
     " out of %d processors\n",
     processor_name, world_rank, world_size);
  MPI_Finalize();
}
\end{lstlisting}

\subsection{Loading OpenMPI}
There are two different version of openMPI available for computing. They are
version 2.1.3 and 3.0.0. Use module tools to load the appropriate version of
the MPI.

\begin{lstlisting}[frame=single]
$ module load openMPI
\end{lstlisting}

\subsection{Compiling}
On the login node or a compute node, the source can be compiled after the
module loaded as:

\begin{lstlisting}[frame=single]
$ mpicc -o mpi_hw mpi_hw.c
\end{lstlisting}

\subsection{Running the compiled code}
No one should run an MPI code directly in the HPC. Use batch script to submit
as a job on the system.

\subsection{Running MPI in batch}
Make a Slurm job script named mpi\_hw.sh with the following contents.

mpi\_hw.sh

\begin{lstlisting}[frame=single]
#!/bin/bash
#SBATCH --node=2
#SBATCH --job-name=mpi_hw
#SBATCH --output=mpi_hw

module load openMPI

mpicc -o mpi_hw mpi_hw.c

mpirun ./mpi_hw
\end{lstlisting}


\subsection{Submitting job in Queue}

\begin{lstlisting}[frame=single]
$ sbatch mpi_hw.sh
\end{lstlisting}

\subsection{Useful Links for openMPI}

\begin{itemize}
    \item \href{https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwiC3oqOga_aAhXCvY8KHXJYCqwQFggnMAA&url=http%3A%2F%2Fmpitutorial.com%2Ftutorials%2F&usg=AOvVaw3gcXG-8XhYsVPu5egCZ2EP} {MPI tutorial}

\item \href{http://mpitutorial.com/tutorials/mpi-hello-world/}{MPI tutorial}
\end{itemize}

\section{Script for Slurm Job Submission}
The job flags are used with SBATCH command.  The syntax for the SLURM directive
in a script is  "\#SBATCH $<flag>$".  Some of the flags are used with the srun and
salloc commands, as well as the fisbatch wrapper script for interactive jobs.

\begin{table}[]
\centering
\caption{Flags for slurm job submission}
\label{table:slurm-job}
    \begin{tabular}{|l|p{3cm}|p{4cm}|p{4.5cm}|}
\hline
        \textbf{
            Resource }  & \textbf{
    Flag Syntax     }            & \textbf{
    Description}
        & \textbf{
Notes                       }         \\ \hline
partition  & --partition=general-compute & Partition is a queue for jobs.                             & default on ub-hpc is general-compute \\ \hline
qos        & --qos=general-compute       & QOS is quality of service value (limits or priority boost) & default on ub-hpc is general-compute \\ \hline
time       & --time=01:00:00             & Time limit for the job.                                    & 1 hour; default is 72 hours          \\ \hline
nodes      & --nodes=2                   & Number of compute nodes for the job.                       & default is 1;  compute nodes         \\ \hline
cpus/cores & --ntasks-per-node=8         & Corresponds to number of cores on the compute node.        & default is 1                         \\ \hline

node type	& --constraint=IB 
or
        --constraint=IB\&CPU-E564 & 
Node type feature. 
        IB requests nodes with InfiniBand &
default is no node type specified;  compute nodes \\ \hline
        resource feature	& --gres=gpu:2	& Request use of GPUs on compute
        nodes	& default is no feature specified; \\ \hline
        memory &
        --mem=24000	& Memory limit per compute node for the  job.  Do not use
        with mem-per-cpu flag. &	memory in MB; default limit is 3000MB per
        core \\ \hline
        memory	& --mem-per-cpu=4000	& Per core memory limit.  Do not use
        the mem flag, &	memory in MB; default limit is 3000MB per core \\
        \hline
        account	& --account=group-slurm-account	& Users may belong to groups or
        accounts.	& default is the user's primary group. \\ \hline
        job name	& --job-name="hello\_test"	& Name of job.	& default is
        the JobID \\ \hline
        output file	& --output=test.out	& Name of file for stdout.	& default
        is the JobID \\ \hline
        email address	& --mail-user=username\@buffalo.edu	& User's email
        address	& required \\ \hline
        email notification	& --mail-type=ALL
        --mail-type=END &
        When email is sent to user.	& omit for no email \\ \hline
        access	& --exclusive	& Exclusive acccess to compute nodes.	&
        default is sharing nodes \\ \hline


\end{tabular}
\end{table}

\section{HPC Sample Job: LAMMPS}
\subsection{Overview}
\subsection{Loading LAMMPS}
\subsection{Running LAMMPS}
\subsection{Running LAMMPS in Batch}
\subsection{Submitting job in Queue}


\section{HPC Sample Job: Gaussian (g09)}
\subsection{Overview}
A Gaussian job submission in HPC can be varied from very simple script to
complicated one. This document will shows only for the simple script that will
run in the YZ-HPC. By using the table \ref{table:slurm-job}, one can obtain the
most useful way of using HPC. Gaussian job can be submitted in both interactive
mode and batch mode. But it is highly recommended that \textbf{a Gaussian job should be
submitted in batch mode (by using the SLURM)}.

\textit{NOTE: Before doing anything, a user must confirm that the available package
of Gaussian is accessible by the user.}

\subsection{Running Gaussian (g09) in Interactive Mode}
\subsubsection{Loading Gaussian (g09)}
As of now, the available package of Gaussian is "Gaussian 09" in the YZ-HPC.
For the inter processor communication, it use linda package. A user must load
the package before using Gaussian. This package can be loaded by the following
command.

\begin{lstlisting}[frame=single]
$ module load gaussian
\end{lstlisting}

\subsubsection{Running Gaussian (g09)}
Suppose, a Gaussian input script is avaible for running which is given below:

One can run this job interactively by:

\begin{lstlisting}[frame=single]
$ g09 gaussianinput.com
\end{lstlisting}

\subsection{Running Gaussian (g09) in Batch}
Consider that you have got a g09 input file called test001.com as:

\begin{lstlisting}[frame=single]
#P TEST STO-3G COMPLEX pop=full scf=tight

Gaussian Test Job 01
SINGLET DELTA STO-3G//STO-3G DIOXYGEN

 0 1
 O
 O 1 R

R 1.220

\end{lstlisting}

You can transfer this file into your HPC account as described in section
\ref{section_transfer}. Or you can write it in the HPC as described in section
\ref{section_editor}. That's how you got an input file (test001.com) for the first run
in HPC. But this file will not run in the HPC environment. To make it usable in
the HPC, modify this file as:

(Do not use backslash sign before percent sign)
\begin{lstlisting}[frame=single]
\%NProcLinda=4
\%NProcShared=4
\%Mem=2800MB
#P TEST STO-3G COMPLEX pop=full scf=tight

Gaussian Test Job 01
SINGLET DELTA STO-3G//STO-3G DIOXYGEN

 0 1
 O
 O 1 R

R 1.220
\end{lstlisting}

This input script will run in the HPC.
For running Gaussian in batch mode, a batch script must be written for the
slurm as gaussian.sh.

gaussian.sh
\begin{lstlisting}[frame=single]
#!/bin/bash -l
#SBATCH -J g09-linda
#
#SBATCH -t 2:00:0
#
#SBATCH -p y_comp -n 16
#SBATCH -A test001.com

export OMP_NUM_THREADS=1

g09 test001.com test001.out
\end{lstlisting}


\subsection{Submitting job in Queue}
With all the required file (in this case, test001.com and gaussian.sh) in
the same directory, run the following command.

\begin{lstlisting}[frame=single]
$ sbatch gaussian.sh
\end{lstlisting}

After completing the job, you will obtain the output in a file name
test001.out.

\end{document}
